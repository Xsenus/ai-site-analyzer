# AI Site Analyzer

AI Site Analyzer — это асинхронный сервис на FastAPI, который анализирует сайты и
возвращает структурированный отчёт. Сервис работает поверх двух PostgreSQL
баз, обращается к OpenAI для генерации отчётов и эмбеддингов, а также
предоставляет вспомогательный эндпоинт `/api/ai-search` для получения
векторных представлений текстов.

## Основные возможности

- **Анализ сайтов по `pars_id` или домену.** Основные эндпоинты располагаются в
  `app/api/routes.py` и принимают запросы `POST /v1/analyze/{pars_id}` и
  `POST /v1/analyze/by-site/{site}`.
- **Поддержка двух баз данных.** Используются асинхронные движки SQLAlchemy с
  ленивой инициализацией и отдельными DSN для `postgres` и `parsing_data`.
- **Интеграция с OpenAI.** Модуль `app/services/analyzer.py` собирает промпт,
  вызывает LLM и разбирает ответ, а `app/services/embeddings.py` отвечает за
  получение эмбеддингов (внутренний сервис → OpenAI fallback).
- **Сервис эмбеддингов `/api/ai-search`.** Реализован в
  `app/routers/ai_search.py`, включает простой rate limit, нормализацию запросов
  и возможность подключить внутренний провайдер эмбеддингов.
- **Единая конфигурация через `.env`.** Настройки описаны в `app/config.py` и
  автоматически нормализуются (алиасы, значения по умолчанию, computed-поля).

## Структура проекта

```text
app/
├── api/                # публичные REST-роуты и pydantic-схемы
├── db/                 # создание движков, транзакции и пинг баз данных
├── models/             # справочники/модели домена
├── repositories/       # работа с БД (чтение текстов, поиск pars_id)
├── routers/            # дополнительные роутеры (ai-search)
├── schemas/            # pydantic-схемы для ai-search
├── services/           # интеграция с OpenAI, логика анализа
├── utils/              # вспомогательные утилиты (rate limiter и др.)
├── config.py           # pydantic Settings для загрузки окружения
├── logging_setup.py    # базовая настройка логирования
└── main.py             # точка входа FastAPI-приложения
```

## Подготовка окружения

1. **Python.** Требуется Python 3.11+.
2. **Зависимости.** Установите пакеты: `pip install -r requirements.txt`.
3. **Переменные окружения.** Скопируйте пример и укажите свои значения:
   ```bash
   cp .env.example .env
   ```
   Минимальный набор — DSN для PostgreSQL и ключ OpenAI. Полный список
   переменных приведён ниже.
4. **Запуск.** Используйте любой из вариантов:
   ```bash
   uvicorn app.main:app --host 0.0.0.0 --port 8090 --reload
   # или
   python -m app.run
   ```

После запуска основное приложение доступно на `http://localhost:8090`.

## Ключевые переменные `.env`

| Переменная | Назначение |
| --- | --- |
| `POSTGRES_URL` | DSN основной БД (`postgresql+psycopg://…`). |
| `PARSING_URL` | DSN БД `parsing_data`. |
| `ECHO_SQL` | Логирование SQL (true/false). |
| `OPENAI_API_KEY` | Ключ OpenAI для генерации отчётов и эмбеддингов. |
| `CHAT_MODEL` | Модель диалога для анализа (по умолчанию `gpt-4o`). |
| `OPENAI_EMBED_MODEL` | Модель эмбеддингов (по умолчанию `text-embedding-3-large`). |
| `INTERNAL_EMBED_URL` | URL внутреннего сервиса эмбеддингов (опционально). |
| `AI_SEARCH_TIMEOUT` | Таймаут `POST /api/ai-search` в секундах. |
| `AI_SEARCH_RATE_LIMIT_PER_MIN` | Лимит запросов `/api/ai-search` в минуту. |
| `DEFAULT_WRITE_MODE` | Режим записи (`primary_only`, `dual_write`, `fallback_to_secondary`). |
| `CORS_ALLOW_*` | Настройки CORS (origins, methods, headers, credentials). |
| `LOG_LEVEL` | Уровень логирования FastAPI-приложения. |

Дополнительные параметры (например, `VECTOR_DIM`, `EMBED_BATCH_SIZE`,
`EMBED_MAX_CHARS`, `DEBUG_OPENAI_LOG`) можно оставить по умолчанию или
переопределить при необходимости.

## Проверка работоспособности

- `GET /health` — проверка подключения к базам.
- `POST /v1/analyze/{pars_id}` — анализ по идентификатору.
- `POST /v1/analyze/by-site/{site}` — анализ по домену.
- `GET /api/ai-search/health` — health чек сервиса эмбеддингов.
- `POST /api/ai-search` — получение эмбеддинга или fallback-ответов.

## Работа с базой данных

Основной эндпоинт `POST /v1/analyze/{pars_id}` по-прежнему напрямую пишет
результаты в PostgreSQL. После этапа обогащения сервис создаёт транзакцию и
последовательно:

1. Гарантирует наличие необходимых колонок и строки в `pars_site`.
2. Обновляет описание и вектор `pars_site` (если эмбеддинг получен).
3. Записывает классификацию (`ai_site_prodclass`) и результаты обогащения
   (`ai_site_equipment_enriched`, `ai_site_goods_types_enriched`).

Запись выполняется через репозиторий `app/repositories/parsing_repo.py` внутри
асинхронной функции `write_all_action`, вызываемой в зависимости от режима
синхронизации (`primary_only`, `dual_write`, `fallback_to_secondary`). Код, который
управляет этой логикой, расположен в `app/api/routes.py` (см. `_analyze_impl`).

Если необходимо получить только JSON без записи в БД, используйте эндпоинт
`POST /v1/analyze/json`. Он выполняет те же шаги анализа, но возвращает
структурированный ответ и сформированный payload для стороннего сервиса, который
может самостоятельно записать данные.

## Запись в зеркальную БД

В режиме `dual_write` сервис последовательно записывает данные в основную и
зеркальную базы. Если в зеркальной базе столбец `pars_site.company_id`
объявлен как `NOT NULL` без значения по умолчанию, необходимо явно передавать
идентификатор компании в запросе анализа:

```json
{
  "company_id": 123,
  "sync_mode": "dual_write"
}
```

Поле `company_id` добавлено в схему `AnalyzeRequest` и используется при вставке
в `pars_site`. Если значение не передано, а зеркальная база требует его
обязательного заполнения, запись будет пропущена, что отразится в отчёте.

## Полезные советы

- Логирование уже настроено в `app/logging_setup.py`. При необходимости
  используйте переменную `LOG_LEVEL` или переопределите формат.
- В продакшене стоит подключить внешнее управление миграциями (Alembic) и
  секретами (Vault, AWS Secrets Manager и т.д.).
- Для локальной разработки можно настроить docker-compose с PostgreSQL и
  пробросить DSN через `.env`.
